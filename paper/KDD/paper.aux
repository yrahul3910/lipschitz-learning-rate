\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{cauchy1847methode}
\citation{kuzborskij2017data}
\citation{hardt2015train}
\citation{zou2018stochastic}
\citation{du2018gradient}
\citation{sutskever2013importance}
\citation{tieleman2012lecture}
\citation{kingma2014adam}
\citation{duchi2011adaptive}
\citation{radford2015unsupervised}
\citation{xu2015show}
\citation{bahar2017empirical}
\citation{broyden1970convergence}
\citation{fletcher1970new}
\citation{goldfarb1970family}
\citation{shanno1970conditioning}
\citation{liu1989limited}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Least squares cost function}{2}{section.3}}
\newlabel{lstsq}{{3}{2}{Least squares cost function}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}An alternate derivation}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Binary cross-entropy function}{2}{section.4}}
\citation{ng2000cs229}
\@writefile{toc}{\contentsline {section}{\numberline {5}Cross-entropy loss function}{3}{section.5}}
\newlabel{softmax:1}{{1}{3}{Cross-entropy loss function}{equation.5.1}{}}
\citation{tsanas2012accurate}
\citation{fernandes2015proactive}
\newlabel{softmax:2}{{2}{4}{Cross-entropy loss function}{equation.5.2}{}}
\newlabel{softmax:3}{{3}{4}{Cross-entropy loss function}{equation.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}A note on regularization}{4}{section.6}}
\newlabel{regularization}{{6}{4}{A note on regularization}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{4}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Loss function over iterations for California housing prices dataset\relax }}{4}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:leastsq:1}{{1}{4}{Loss function over iterations for California housing prices dataset\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Regression experiments}{4}{subsection.7.1}}
\newlabel{regexpts}{{7.1}{4}{Regression experiments}{subsection.7.1}{}}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Binary classification experiments}{5}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Multi-class classification experiments}{5}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}MNIST}{5}{subsubsection.7.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of training and validation accuracy scores for different $\alpha $ on MNIST\relax }}{5}{figure.caption.11}}
\newlabel{fig:classif:1}{{2}{5}{Comparison of training and validation accuracy scores for different $\alpha $ on MNIST\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of training and validation accuracy scores for same $\alpha $ on MNIST\relax }}{5}{figure.caption.12}}
\newlabel{fig:classif:2}{{3}{5}{Comparison of training and validation accuracy scores for same $\alpha $ on MNIST\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion and Future Work}{5}{section.8}}
\bibstyle{ACM-Reference-Format}
\bibdata{citations}
\bibcite{bahar2017empirical}{{1}{2017}{{Bahar et~al\unhbox \voidb@x \hbox {.}}}{{Bahar, Alkhouli, Peter, Brix, and Ney}}}
\bibcite{broyden1970convergence}{{2}{1970}{{Broyden}}{{Broyden}}}
\bibcite{cauchy1847methode}{{3}{1847}{{Cauchy}}{{Cauchy}}}
\bibcite{du2018gradient}{{4}{2018}{{Du et~al\unhbox \voidb@x \hbox {.}}}{{Du, Zhai, Poczos, and Singh}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Regression experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac  {1}{L}$\relax }}{6}{table.caption.6}}
\newlabel{tab:leastsq:1}{{1}{6}{Regression experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac {1}{L}$\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Binary classification experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac  {1}{L}$\relax }}{6}{table.caption.7}}
\newlabel{tab:classif:1}{{2}{6}{Binary classification experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac {1}{L}$\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Softmax classification experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac  {1}{L}$\relax }}{6}{table.caption.8}}
\newlabel{tab:classif:2}{{3}{6}{Softmax classification experiments on various datasets with $\alpha =0.1$ and $\alpha =\frac {1}{L}$\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Classification experiments on various datasets with an error threshold\relax }}{6}{table.caption.9}}
\newlabel{tab:classif:3}{{4}{6}{Classification experiments on various datasets with an error threshold\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Iterations required to reach accuracy thresholds in MNIST experiments\relax }}{6}{table.caption.10}}
\newlabel{tab:classif:4}{{5}{6}{Iterations required to reach accuracy thresholds in MNIST experiments\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.14}}
\bibcite{duchi2011adaptive}{{5}{2011}{{Duchi et~al\unhbox \voidb@x \hbox {.}}}{{Duchi, Hazan, and Singer}}}
\bibcite{fernandes2015proactive}{{6}{2015}{{Fernandes et~al\unhbox \voidb@x \hbox {.}}}{{Fernandes, Vinagre, and Cortez}}}
\bibcite{fletcher1970new}{{7}{1970}{{Fletcher}}{{Fletcher}}}
\bibcite{goldfarb1970family}{{8}{1970}{{Goldfarb}}{{Goldfarb}}}
\bibcite{hardt2015train}{{9}{2015}{{Hardt et~al\unhbox \voidb@x \hbox {.}}}{{Hardt, Recht, and Singer}}}
\bibcite{softmax}{{10}{2016}{{Juliani}}{{Juliani}}}
\bibcite{kingma2014adam}{{11}{2014}{{Kingma and Ba}}{{Kingma and Ba}}}
\bibcite{kuzborskij2017data}{{12}{2017}{{Kuzborskij and Lampert}}{{Kuzborskij and Lampert}}}
\bibcite{liu1989limited}{{13}{1989}{{Liu and Nocedal}}{{Liu and Nocedal}}}
\bibcite{ng2000cs229}{{14}{2000}{{Ng}}{{Ng}}}
\bibcite{scikit-learn}{{15}{2011}{{Pedregosa et~al\unhbox \voidb@x \hbox {.}}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{radford2015unsupervised}{{16}{2015}{{Radford et~al\unhbox \voidb@x \hbox {.}}}{{Radford, Metz, and Chintala}}}
\bibcite{shanno1970conditioning}{{17}{1970}{{Shanno}}{{Shanno}}}
\bibcite{sutskever2013importance}{{18}{2013}{{Sutskever et~al\unhbox \voidb@x \hbox {.}}}{{Sutskever, Martens, Dahl, and Hinton}}}
\bibcite{tieleman2012lecture}{{19}{2012}{{Tieleman and Hinton}}{{Tieleman and Hinton}}}
\bibcite{tsanas2012accurate}{{20}{2012}{{Tsanas and Xifara}}{{Tsanas and Xifara}}}
\bibcite{xu2015show}{{21}{2015}{{Xu et~al\unhbox \voidb@x \hbox {.}}}{{Xu, Ba, Kiros, Cho, Courville, Salakhudinov, Zemel, and Bengio}}}
\bibcite{zou2018stochastic}{{22}{2018}{{Zou et~al\unhbox \voidb@x \hbox {.}}}{{Zou, Cao, Zhou, and Gu}}}
\citation{softmax}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Experiment Results}{7}{appendix.A}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Code Snippets}{7}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Regression experiments}{7}{subsection.B.1}}
\@writefile{lol}{\contentsline {lstlisting}{code/reg\textunderscore exp.py}{7}{lstlisting.-1}}
\newlabel{fig:1:a}{{4a}{7}{$\alpha =0.1$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:1:a}{{a}{7}{$\alpha =0.1$\relax }{figure.caption.15}{}}
\newlabel{fig:1:b}{{4b}{7}{$\alpha =1/L$\relax }{figure.caption.15}{}}
\newlabel{sub@fig:1:b}{{b}{7}{$\alpha =1/L$\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Precision, recall, and F1-scores for different learning rates on the digits data\relax }}{7}{figure.caption.15}}
\newlabel{fig:1}{{4}{7}{Precision, recall, and F1-scores for different learning rates on the digits data\relax }{figure.caption.15}{}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{11.45699pt}
\newlabel{tocindent3}{18.198pt}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Classification experiments}{8}{subsection.B.2}}
\@writefile{lol}{\contentsline {lstlisting}{code/softmax.py}{8}{lstlisting.-2}}
\@writefile{lol}{\contentsline {lstlisting}{code/softmax\textunderscore main.py}{8}{lstlisting.-3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}MNIST}{8}{subsection.B.3}}
\@writefile{lol}{\contentsline {lstlisting}{code/mnist\textunderscore pre.py}{8}{lstlisting.-4}}
\@writefile{lol}{\contentsline {lstlisting}{code/mnist\textunderscore model.py}{8}{lstlisting.-5}}
\newlabel{TotPages}{{8}{8}{}{page.8}{}}
