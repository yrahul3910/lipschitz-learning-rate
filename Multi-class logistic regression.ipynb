{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 1.535794942750403\n"
     ]
    }
   ],
   "source": [
    "m = y.shape[0]\n",
    "L = 1 / m * np.linalg.norm(x)\n",
    "print('Learning rate =', 1/L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax1 = SGDClassifier(loss='log',               # log-loss for logistic regression\n",
    "                        penalty='none',           # do not regularize...yet\n",
    "                        learning_rate='constant', # need to set\n",
    "                        shuffle=False,            # for consistency with our code\n",
    "                        eta0=1/L,                 # learning rate\n",
    "                        max_iter=10,\n",
    "                        verbose=1,\n",
    "                        tol=1e-7\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 10.73, NNZs: 4, Bias: -0.767897, T: 105, Avg. loss: 0.378191\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.78, NNZs: 4, Bias: -0.881070, T: 210, Avg. loss: 2.137158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.07, NNZs: 4, Bias: -0.865826, T: 315, Avg. loss: 2.100217\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.21, NNZs: 4, Bias: 0.147788, T: 420, Avg. loss: 2.248002\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 15.71, NNZs: 4, Bias: 0.147791, T: 525, Avg. loss: 0.370149\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 13.58, NNZs: 4, Bias: 0.147791, T: 630, Avg. loss: 2.267239\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 11.64, NNZs: 4, Bias: 0.127120, T: 735, Avg. loss: 2.225976\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 16.55, NNZs: 4, Bias: 0.127153, T: 840, Avg. loss: 1.999917\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 14.52, NNZs: 4, Bias: 0.127153, T: 945, Avg. loss: 2.269580\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 12.57, NNZs: 4, Bias: 0.125594, T: 1050, Avg. loss: 2.237420\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 10 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.37, NNZs: 4, Bias: -0.767898, T: 105, Avg. loss: 1.189895\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.51, NNZs: 4, Bias: -1.861609, T: 210, Avg. loss: 1.157271\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.64, NNZs: 4, Bias: -1.804473, T: 315, Avg. loss: 1.292443\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.52, NNZs: 4, Bias: -1.804472, T: 420, Avg. loss: 1.224880\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.96, NNZs: 4, Bias: -1.804472, T: 525, Avg. loss: 1.186996\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 9.07, NNZs: 4, Bias: -1.804472, T: 630, Avg. loss: 1.149113\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 9.81, NNZs: 4, Bias: -1.804478, T: 735, Avg. loss: 1.111230\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 18.99, NNZs: 4, Bias: -3.331492, T: 840, Avg. loss: 1.110551\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 20.77, NNZs: 4, Bias: -3.328612, T: 945, Avg. loss: 2.429210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 15.36, NNZs: 4, Bias: -2.051464, T: 1050, Avg. loss: 2.437647\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8.60, NNZs: 4, Bias: 0.767897, T: 105, Avg. loss: 0.330947\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.49, NNZs: 4, Bias: 0.768267, T: 210, Avg. loss: 2.191576\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.70, NNZs: 4, Bias: 0.926527, T: 315, Avg. loss: 2.143306\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.48, NNZs: 4, Bias: 0.926527, T: 420, Avg. loss: 2.138477\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.10, NNZs: 4, Bias: -0.129368, T: 525, Avg. loss: 2.254088\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 12.72, NNZs: 4, Bias: -0.129368, T: 630, Avg. loss: 0.406334\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 6 epochs took 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=1.535794942750403,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "       loss='log', max_iter=10, n_iter=None, n_iter_no_change=5,\n",
       "       n_jobs=None, penalty='none', power_t=0.5, random_state=None,\n",
       "       shuffle=False, tol=1e-07, validation_fraction=0.1, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax2 = SGDClassifier(loss='log',               # log-loss for logistic regression\n",
    "                        penalty='none',           # do not regularize...yet\n",
    "                        learning_rate='constant', # need to set\n",
    "                        shuffle=False,            # for consistency with our code\n",
    "                        eta0=0.1,                 # learning rate\n",
    "                        max_iter=10,\n",
    "                        verbose=1,\n",
    "                        tol=1e-7\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.85, NNZs: 4, Bias: -0.075554, T: 105, Avg. loss: 0.090977\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.86, NNZs: 4, Bias: -0.071475, T: 210, Avg. loss: 0.166510\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.87, NNZs: 4, Bias: -0.068113, T: 315, Avg. loss: 0.166812\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.88, NNZs: 4, Bias: -0.065326, T: 420, Avg. loss: 0.166941\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.90, NNZs: 4, Bias: -0.062955, T: 525, Avg. loss: 0.167004\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.91, NNZs: 4, Bias: -0.060894, T: 630, Avg. loss: 0.167013\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.63, NNZs: 4, Bias: -0.085554, T: 105, Avg. loss: 0.163651\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.62, NNZs: 4, Bias: -0.074886, T: 210, Avg. loss: 0.156728\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.63, NNZs: 4, Bias: -0.067930, T: 315, Avg. loss: 0.155647\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.65, NNZs: 4, Bias: -0.063121, T: 420, Avg. loss: 0.154915\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.69, NNZs: 4, Bias: -0.059490, T: 525, Avg. loss: 0.154320\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.73, NNZs: 4, Bias: -0.056486, T: 630, Avg. loss: 0.153788\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.78, NNZs: 4, Bias: -0.053797, T: 735, Avg. loss: 0.153291\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.83, NNZs: 4, Bias: -0.051244, T: 840, Avg. loss: 0.152820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.88, NNZs: 4, Bias: -0.048724, T: 945, Avg. loss: 0.152370\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.94, NNZs: 4, Bias: -0.046175, T: 1050, Avg. loss: 0.151939\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 0.70, NNZs: 4, Bias: 0.029806, T: 105, Avg. loss: 0.089520\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.86, NNZs: 4, Bias: 0.012042, T: 210, Avg. loss: 0.137603\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.90, NNZs: 4, Bias: -0.016534, T: 315, Avg. loss: 0.148078\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.97, NNZs: 4, Bias: -0.039228, T: 420, Avg. loss: 0.144341\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.03, NNZs: 4, Bias: -0.059808, T: 525, Avg. loss: 0.143937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.09, NNZs: 4, Bias: -0.077134, T: 630, Avg. loss: 0.143340\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='constant', loss='log', max_iter=10,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='none',\n",
       "       power_t=0.5, random_state=None, shuffle=False, tol=1e-07,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "Custom: 1.0 \n",
      "Normal: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Scores:\\nCustom:', softmax1.score(x_test, y_test), '\\nNormal:', softmax2.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(x, y, alpha=0.1, n_iter=100, regularize=False, lambd=0.1, verbose=0):\n",
    "    np.random.shuffle(x)\n",
    "    \n",
    "    x_norm = np.sum(x, axis=0)\n",
    "    x /= x_norm\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #L = 1 / m * np.linalg.norm(x_train)\n",
    "    L = 1 / m * np.linalg.norm(np.dot(y_train.T, x_train))\n",
    "    penalty = 'none'\n",
    "    \n",
    "    if regularize:\n",
    "        df = pd.DataFrame(x_train)\n",
    "        a = df.mean().sum()\n",
    "        b = df.max().mean()\n",
    "        K = (a + b) / 2\n",
    "        \n",
    "        L += lambd * K\n",
    "        penalty = 'l2'\n",
    "        \n",
    "    \n",
    "    print('Learning rate =', 1/L)\n",
    "    \n",
    "    softmax1 = SGDClassifier(loss='log',               # log-loss for logistic regression\n",
    "                             penalty=penalty,          # do not regularize...yet\n",
    "                             alpha=lambd,\n",
    "                             learning_rate='constant', # need to set\n",
    "                             shuffle=False,            # for consistency with our code\n",
    "                             eta0=alpha,              # learning rate\n",
    "                             max_iter=n_iter,\n",
    "                             verbose=verbose,\n",
    "                             tol=1e-7\n",
    "                            )\n",
    "    softmax2 = SGDClassifier(loss='log',               # log-loss for logistic regression\n",
    "                             penalty=penalty,          # do not regularize...yet\n",
    "                             alpha=lambd,\n",
    "                             learning_rate='constant', # need to set\n",
    "                             shuffle=False,            # for consistency with our code\n",
    "                             eta0=1/L,                 # learning rate\n",
    "                             max_iter=n_iter,\n",
    "                             verbose=verbose,\n",
    "                             tol=1e-7\n",
    "                            )\n",
    "    \n",
    "    softmax1.fit(x_train, y_train)\n",
    "    softmax2.fit(x_train, y_train)\n",
    "    \n",
    "    print('Scores:\\nCustom:', softmax1.score(x_test, y_test), '\\nNormal:', softmax2.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1797 entries, 0 to 1796\n",
      "Data columns (total 64 columns):\n",
      "0     0 non-null float64\n",
      "1     1797 non-null float64\n",
      "2     1797 non-null float64\n",
      "3     1797 non-null float64\n",
      "4     1797 non-null float64\n",
      "5     1797 non-null float64\n",
      "6     1797 non-null float64\n",
      "7     1797 non-null float64\n",
      "8     1797 non-null float64\n",
      "9     1797 non-null float64\n",
      "10    1797 non-null float64\n",
      "11    1797 non-null float64\n",
      "12    1797 non-null float64\n",
      "13    1797 non-null float64\n",
      "14    1797 non-null float64\n",
      "15    1797 non-null float64\n",
      "16    1797 non-null float64\n",
      "17    1797 non-null float64\n",
      "18    1797 non-null float64\n",
      "19    1797 non-null float64\n",
      "20    1797 non-null float64\n",
      "21    1797 non-null float64\n",
      "22    1797 non-null float64\n",
      "23    1797 non-null float64\n",
      "24    1797 non-null float64\n",
      "25    1797 non-null float64\n",
      "26    1797 non-null float64\n",
      "27    1797 non-null float64\n",
      "28    1797 non-null float64\n",
      "29    1797 non-null float64\n",
      "30    1797 non-null float64\n",
      "31    1797 non-null float64\n",
      "32    0 non-null float64\n",
      "33    1797 non-null float64\n",
      "34    1797 non-null float64\n",
      "35    1797 non-null float64\n",
      "36    1797 non-null float64\n",
      "37    1797 non-null float64\n",
      "38    1797 non-null float64\n",
      "39    0 non-null float64\n",
      "40    1797 non-null float64\n",
      "41    1797 non-null float64\n",
      "42    1797 non-null float64\n",
      "43    1797 non-null float64\n",
      "44    1797 non-null float64\n",
      "45    1797 non-null float64\n",
      "46    1797 non-null float64\n",
      "47    1797 non-null float64\n",
      "48    1797 non-null float64\n",
      "49    1797 non-null float64\n",
      "50    1797 non-null float64\n",
      "51    1797 non-null float64\n",
      "52    1797 non-null float64\n",
      "53    1797 non-null float64\n",
      "54    1797 non-null float64\n",
      "55    1797 non-null float64\n",
      "56    1797 non-null float64\n",
      "57    1797 non-null float64\n",
      "58    1797 non-null float64\n",
      "59    1797 non-null float64\n",
      "60    1797 non-null float64\n",
      "61    1797 non-null float64\n",
      "62    1797 non-null float64\n",
      "63    1797 non-null float64\n",
      "dtypes: float64(64)\n",
      "memory usage: 898.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014260061634596132\n",
      "Learning rate = 52.94814761122967\n",
      "Scores:\n",
      "Custom: 0.09444444444444444 \n",
      "Normal: 0.1037037037037037\n"
     ]
    }
   ],
   "source": [
    "run_experiment(X, Y, n_iter=500, regularize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
